# VOICE CHAT WEBSOCKET FLOW 

Frontend (React)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1. Record audio chunks        ‚îÇ
‚îÇ 2. Send binary frames         ‚îÇ
‚îÇ 3. On pause: send JSON event  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ WebSocket (WSS)
              ‚ñº
Backend (Go)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1. Receives audio chunks     ‚îÇ
‚îÇ 2. Forwards to AI/ML WS      ‚îÇ
‚îÇ 3. Receives streamed AI text ‚îÇ
‚îÇ 4. Receives streamed TTS     ‚îÇ
‚îÇ 5. Relays text/audio to FE   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ WebSocket (internal)
              ‚ñº
AI/ML Service (Python)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ          ‚îÄ‚îê
‚îÇ 1. Receive audio chunks                ‚îÇ
‚îÇ 2. STT engine ‚Üí partial/final text     ‚îÇ
‚îÇ 3. LLM engine ‚Üí generate text response ‚îÇ
‚îÇ 4. TTS engine ‚Üí audio chunks           ‚îÇ
‚îÇ 5. Stream text/audio back              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ          ‚îò


---

## üéôÔ∏è VOICE CHAT API DOCUMENTATION

### Base URLs

| Environment | URL                                    |
| ----------- | -------------------------------------- |
| Development | `http://localhost:8080/api/v1`         |
| Production  | `https://api.arogyasahayak.com/api/v1` |

---

## 1. üéß Voice Chat Session Lifecycle

| Step | Description                                                                         |
| ---- | ----------------------------------------------------------------------------------- |
| 1    | Frontend requests to **start a voice session**                                      |
| 2    | Backend issues a **session ID** and establishes a **WebSocket** for streaming       |
| 3    | Frontend records audio chunks (PCM or Opus) and sends via WebSocket to backend      |
| 4    | Backend forwards chunks over another WebSocket to **AI/ML service**                 |
| 5    | AI/ML service streams **partial transcriptions (STT)** ‚Üí forwards to AI model       |
| 6    | AI/ML service streams **AI text responses** ‚Üí converts them to **TTS audio chunks** |
| 7    | Backend relays the streamed **audio output chunks** to frontend                     |
| 8    | Frontend plays them in real-time (like ChatGPT Voice)                               |

---

## 2. üöÄ API ENDPOINTS ‚Äî BACKEND (Go)

### `POST /voice/session/start`

Start a new voice chat session.

**Request**

```json
{
  "language": "hi",
  "model": "mistral-7b",
  "session_type": "voice"
}
```

**Response (200)**

```json
{
  "session_id": "vsn_8df91e",
  "ws_url": "wss://api.arogyasahayak.com/api/v1/voice/session/vsn_8df91e/ws"
}
```

---

### `GET /voice/session/:id/ws` (WebSocket)

Bi-directional streaming endpoint for audio and AI responses.

#### üîÑ WebSocket Message Types

**Client ‚Üí Server**

| Type           | Description                                | Example                                                       |
| -------------- | ------------------------------------------ | ------------------------------------------------------------- |
| `audio_chunk`  | Binary PCM/Opus data                       | (binary data)                                                 |
| `end_of_input` | Marks user finished speaking               | `{"type": "end_of_input"}`                                    |
| `text_message` | Optional: Send text query instead of voice | `{"type": "text_message", "content": "Show my blood report"}` |

**Server ‚Üí Client**

| Type                 | Description                     | Example                                                                     |
| -------------------- | ------------------------------- | --------------------------------------------------------------------------- |
| `partial_transcript` | STT partial result              | `{"type": "partial_transcript", "text": "show me my"}`                      |
| `final_transcript`   | Full user query after pause     | `{"type": "final_transcript", "text": "show me my blood report"}`           |
| `ai_text`            | AI model streamed text response | `{"type": "ai_text", "text": "Here‚Äôs what your blood report indicates..."}` |
| `ai_audio`           | AI response audio chunks (TTS)  | (binary audio data)                                                         |
| `end_of_response`    | Marks end of AI response        | `{"type": "end_of_response"}`                                               |

---

### `POST /voice/session/end`

End the current voice session manually.

**Request**

```json
{ "session_id": "vsn_8df91e" }
```

**Response**

```json
{ "message": "Session ended successfully" }
```

---

## 3. üß† API ENDPOINTS ‚Äî AI/ML SERVICE (Python)

> All streaming handled via WebSocket between Backend ‚Üî AI/ML Service.

### `POST /internal/ai/session/start`

Called by Go backend to initialize AI pipeline (STT ‚Üí AI ‚Üí TTS).

**Request**

```json
{
  "session_id": "vsn_8df91e",
  "language": "hi",
  "model": "mistral-7b"
}
```

**Response**

```json
{
  "session_id": "vsn_8df91e",
  "ws_url": "ws://ai-service:8000/session/vsn_8df91e/ws"
}
```

---

### `GET /session/:id/ws` (WebSocket ‚Äî Internal)

Handles real-time AI conversation.

#### Backend ‚Üí AI/ML

| Type           | Description                                     |
| -------------- | ----------------------------------------------- |
| `audio_chunk`  | Raw audio stream from user                      |
| `end_of_input` | Pause detected ‚Üí begin STT ‚Üí AI inference ‚Üí TTS |
| `text_message` | Text query instead of voice                     |

#### AI/ML ‚Üí Backend

| Type                 | Description             |
| -------------------- | ----------------------- |
| `partial_transcript` | Realtime STT            |
| `final_transcript`   | Final STT text          |
| `ai_text`            | AI streamed text output |
| `ai_audio`           | TTS audio chunks        |
| `end_of_response`    | Marks end               |

---

## 4. üé§ AI/ML INTERNAL PIPELINE (Python service)

**Pipeline** inside AI service for every `end_of_input` event:

```
AUDIO CHUNKS
   ‚Üì
STT Model (e.g., Whisper small)
   ‚Üì
Text ‚Üí AI Model (Mistral, Llama, etc.)
   ‚Üì
TTS Model (e.g., VITS / XTTS / Bark / Coqui)
   ‚Üì
Stream audio chunks ‚Üí Backend
```

---

## 5. üß© PROTOCOL SUMMARY

| Layer                   | Protocol  | Direction | Purpose           |
| ----------------------- | --------- | --------- | ----------------- |
| Frontend ‚Üî Backend      | WebSocket | Duplex    | Audio in, TTS out |
| Backend ‚Üî AI/ML Service | WebSocket | Duplex    | Stream STT + TTS  |
| Backend ‚Üî Frontend      | HTTP      | Control   | Start/end session |
| Backend ‚Üî Auth DB       | HTTP/SQL  | -         | JWT, user info    |

---

## 6. üéôÔ∏è AUDIO SPECIFICATIONS

| Type   | Format                 | Sample Rate | Encoding |
| ------ | ---------------------- | ----------- | -------- |
| Input  | 16-bit PCM / Opus      | 16kHz       | mono     |
| Output | 16-bit PCM / MP3 / OGG | 16kHz       | mono     |

---

