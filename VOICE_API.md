# VOICE CHAT WEBSOCKET FLOW 

Frontend (React)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Record audio chunks        â”‚
â”‚ 2. Send binary frames         â”‚
â”‚ 3. On pause: send JSON event  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚ WebSocket (WSS)
              â–¼
Backend (Go)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Receives audio chunks     â”‚
â”‚ 2. Forwards to AI/ML WS      â”‚
â”‚ 3. Receives streamed AI text â”‚
â”‚ 4. Receives streamed TTS     â”‚
â”‚ 5. Relays text/audio to FE   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚ WebSocket (internal)
              â–¼
AI/ML Service (Python)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”€â”
â”‚ 1. Receive audio chunks                â”‚
â”‚ 2. STT engine â†’ partial/final text     â”‚
â”‚ 3. LLM engine â†’ generate text response â”‚
â”‚ 4. TTS engine â†’ audio chunks           â”‚
â”‚ 5. Stream text/audio back              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”˜


---

## ğŸ™ï¸ VOICE CHAT API DOCUMENTATION

### Base URLs

| Environment | URL                                    |
| ----------- | -------------------------------------- |
| Development | `http://localhost:8080/api/v1`         |
| Production  | `https://api.arogyasahayak.com/api/v1` |

---

## 1. ğŸ§ Voice Chat Session Lifecycle

| Step | Description                                                                         |
| ---- | ----------------------------------------------------------------------------------- |
| 1    | Frontend requests to **start a voice session**                                      |
| 2    | Backend issues a **session ID** and establishes a **WebSocket** for streaming       |
| 3    | Frontend records audio chunks (PCM or Opus) and sends via WebSocket to backend      |
| 4    | Backend forwards chunks over another WebSocket to **AI/ML service**                 |
| 5    | AI/ML service streams **partial transcriptions (STT)** â†’ forwards to AI model       |
| 6    | AI/ML service streams **AI text responses** â†’ converts them to **TTS audio chunks** |
| 7    | Backend relays the streamed **audio output chunks** to frontend                     |
| 8    | Frontend plays them in real-time (like ChatGPT Voice)                               |

---

## 2. ğŸš€ API ENDPOINTS â€” BACKEND (Go)

### `POST /voice/session/start`

Start a new voice chat session.

**Request**

Authorization: Bearer <token>

```json
{
  "language": "hi",
  "model": "mistral-7b",
  "session_type": "voice"
}
```

**Response (200)**

```json
{
  "session_id": "vsn_8df91e",
  "ws_url": "wss://api.arogyasahayak.com/api/v1/voice/session/vsn_8df91e/ws"
}
```

---

### `GET /voice/session/:id/ws` (WebSocket)

Bi-directional streaming endpoint for audio and AI responses.

#### ğŸ”„ WebSocket Message Types

**Client â†’ Server**

| Type           | Description                                | Example                                                       |
| -------------- | ------------------------------------------ | ------------------------------------------------------------- |
| `audio_chunk`  | Binary PCM/Opus data                       | (binary data)                                                 |
| `end_of_input` | Marks user finished speaking               | `{"type": "end_of_input"}`                                    |
| `text_message` | Optional: Send text query instead of voice | `{"type": "text_message", "content": "Show my blood report"}` |

**Server â†’ Client**

| Type                 | Description                     | Example                                                                     |
| -------------------- | ------------------------------- | --------------------------------------------------------------------------- |
| `partial_transcript` | STT partial result              | `{"type": "partial_transcript", "text": "show me my"}`                      |
| `final_transcript`   | Full user query after pause     | `{"type": "final_transcript", "text": "show me my blood report"}`           |
| `ai_text`            | AI model streamed text response | `{"type": "ai_text", "text": "Hereâ€™s what your blood report indicates..."}` |
| `ai_audio`           | AI response audio chunks (TTS)  | (binary audio data)                                                         |
| `end_of_response`    | Marks end of AI response        | `{"type": "end_of_response"}`                                               |

---

### `POST /voice/session/end`

End the current voice session manually.

**Request**

```json
{ "session_id": "vsn_8df91e" }
```

**Response**

```json
{ "message": "Session ended successfully" }
```

---

## 3. ğŸ§  API ENDPOINTS â€” AI/ML SERVICE (Python)

> All streaming handled via WebSocket between Backend â†” AI/ML Service.

### `POST /internal/ai/session/start`

Called by Go backend to initialize AI pipeline (STT â†’ AI â†’ TTS).

**Request**

```json
{
  "session_id": "vsn_8df91e",
  "language": "hi",
  "model": "mistral-7b"
}
```

**Response**

```json
{
  "session_id": "vsn_8df91e",
  "ws_url": "ws://ai-service:8000/session/vsn_8df91e/ws"
}
```

---

### `GET /session/:id/ws` (WebSocket â€” Internal)

Handles real-time AI conversation.

#### Backend â†’ AI/ML

| Type           | Description                                     |
| -------------- | ----------------------------------------------- |
| `audio_chunk`  | Raw audio stream from user                      |
| `end_of_input` | Pause detected â†’ begin STT â†’ AI inference â†’ TTS |
| `text_message` | Text query instead of voice                     |

#### AI/ML â†’ Backend

| Type                 | Description             |
| -------------------- | ----------------------- |
| `partial_transcript` | Realtime STT            |
| `final_transcript`   | Final STT text          |
| `ai_text`            | AI streamed text output |
| `ai_audio`           | TTS audio chunks        |
| `end_of_response`    | Marks end               |

---

## 4. ğŸ¤ AI/ML INTERNAL PIPELINE (Python service)

**Pipeline** inside AI service for every `end_of_input` event:

```
AUDIO CHUNKS
   â†“
STT Model (e.g., Whisper small)
   â†“
Text â†’ AI Model (Mistral, Llama, etc.)
   â†“
TTS Model (e.g., VITS / XTTS / Bark / Coqui)
   â†“
Stream audio chunks â†’ Backend
```

---

## 5. ğŸ§© PROTOCOL SUMMARY

| Layer                   | Protocol  | Direction | Purpose           |
| ----------------------- | --------- | --------- | ----------------- |
| Frontend â†” Backend      | WebSocket | Duplex    | Audio in, TTS out |
| Backend â†” AI/ML Service | WebSocket | Duplex    | Stream STT + TTS  |
| Backend â†” Frontend      | HTTP      | Control   | Start/end session |
| Backend â†” Auth DB       | HTTP/SQL  | -         | JWT, user info    |

---

## 6. ğŸ™ï¸ AUDIO SPECIFICATIONS

| Type   | Format                 | Sample Rate | Encoding |
| ------ | ---------------------- | ----------- | -------- |
| Input  | 16-bit PCM / Opus      | 16kHz       | mono     |
| Output | 16-bit PCM / MP3 / OGG | 16kHz       | mono     |

---

